{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "한국어 요약 ai",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1ZYti-gJANfskEY_NYaBE1xGylxs_cB_y",
      "authorship_tag": "ABX9TyMrfSvXK1FkJ/O6bs5wT4sk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gihoonpark/-Korean_News_summarization-NLP/blob/main/%ED%95%9C%EA%B5%AD%EC%96%B4_%EC%9A%94%EC%95%BD_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVBGEfQOL6Za"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMHef9btL1R6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVWYRGWha2eD"
      },
      "source": [
        "Data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmJ1ZvNjrpgY"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/My Drive/dataset/생성요약/train.jsonl', 'r') as json_file:\n",
        "    json_list = list(json_file)\n",
        "with open('/content/drive/MyDrive/dataset/생성요약/abstractive_test_v2.jsonl', 'r') as json_file2:\n",
        "    json_list2 = list(json_file2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnex2j9EqnY0"
      },
      "source": [
        "text_list_train = []\n",
        "summary_list_train = []\n",
        "\n",
        "for json_str in json_list:\n",
        "    result = json.loads(json_str)\n",
        "    text_list_train.append(result.get('article_original'))\n",
        "    summary_list_train.append(result.get('abstractive'))\n",
        "\n",
        "txt_train = pd.Series(text_list_train)\n",
        "summary_train = pd.Series(summary_list_train)\n",
        "\n",
        "txt_col = []\n",
        "for i in range(len(txt_train)):\n",
        "    txt_col.append(\" \".join(txt_train[i]))\n",
        "txt_col = pd.Series(txt_col)\n",
        "train_df = pd.concat([txt_col, summary_train], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6oEpe85a7-X"
      },
      "source": [
        "Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u25c4VcuVFJO"
      },
      "source": [
        "train_df[0] = train_df[0].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "train_df[1] = train_df[1].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "train_df.replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "train_df = train_df.dropna(how='any') # Null 값 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBC-XoKfiw_o"
      },
      "source": [
        "train_df[0] = train_df[0].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9 ]\",\"\") # 정규 표현식 수행\n",
        "train_df[1] = train_df[1].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9 ]\",\"\") # 정규 표현식 수행\n",
        "train_df.replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "train_df = train_df.dropna(how='any') # Null 값 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzzQ7u1nDC8B"
      },
      "source": [
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTpzTIb0WaGt"
      },
      "source": [
        "text_list_test = []\n",
        "\n",
        "for json_str in json_list2:\n",
        "    result2 = json.loads(json_str)\n",
        "    text_list_test.append(result2.get('article_original'))\n",
        "\n",
        "txt_test = pd.Series(text_list_test)\n",
        "txt_col2 = []\n",
        "for i in range(len(txt_test)):\n",
        "    txt_col2.append(\" \".join(txt_test[i]))\n",
        "test_df = pd.Series(txt_col2)\n",
        "summary_test = pd.Series([])\n",
        "test_df = pd.concat([test_df, summary_test], axis=1)\n",
        "\n",
        "test_df[0] = test_df[0].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9 ]\",\"\") # 정규 표현식 수행"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uGSw4G0a_AA"
      },
      "source": [
        "Data tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O-W2GFnVaBd"
      },
      "source": [
        "mecab = Mecab()\n",
        "train_df['X_tokenized'] = train_df[0].apply(mecab.morphs)\n",
        "train_df['y_tokenized'] = train_df[1].apply(mecab.morphs)\n",
        "train_df['X_tokenized'] = train_df['X_tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "train_df['y_tokenized'] = train_df['y_tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "test_df['X_tokenized'] = test_df[0].apply(mecab.morphs)\n",
        "test_df['X_tokenized'] = test_df['X_tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szGhQwrscd3u"
      },
      "source": [
        "# 보류\n",
        "X_train = train_df['X_tokenized'].values\n",
        "y_train = train_df['y_tokenized'].values\n",
        "X_test= test_df['X_tokenized'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aIMuqYS2hiu"
      },
      "source": [
        "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가\n",
        "train_df['decoder_input'] = train_df['y_tokenized'].apply(lambda x : ['sostoken']+ x)\n",
        "train_df['decoder_target'] = train_df['y_tokenized'].apply(lambda x : x + ['eostoken'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_HElWC_2wz_"
      },
      "source": [
        "encoder_input = np.array(train_df['X_tokenized'])\n",
        "decoder_input = np.array(train_df['decoder_input'])\n",
        "decoder_target = np.array(train_df['decoder_target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PxMWYrUc9b7"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(encoder_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3FjJkYSdOyI"
      },
      "source": [
        "'''\n",
        "threshold = 5\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "datS3YGodXlr"
      },
      "source": [
        "src_vocab = 42000\n",
        "src_tokenizer = Tokenizer(num_words = src_vocab) \n",
        "src_tokenizer.fit_on_texts(encoder_input)\n",
        "\n",
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input) \n",
        "encoder_input_test = src_tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7o4jqGF8kj7"
      },
      "source": [
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(decoder_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzEELHLQ8kmv"
      },
      "source": [
        "'''\n",
        "threshold = 6\n",
        "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tar_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzArPscJ8kpn"
      },
      "source": [
        "tar_vocab = 15000\n",
        "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
        "tar_tokenizer.fit_on_texts(decoder_input)\n",
        "tar_tokenizer.fit_on_texts(decoder_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ7KTCqO83Jx"
      },
      "source": [
        "# model에 들어갈 최종 dataset\n",
        "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input) \n",
        "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target)\n",
        "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3yNYKVRbL0S"
      },
      "source": [
        "Data padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw1-IPObheVK"
      },
      "source": [
        "text_max_len = 510\n",
        "summary_max_len = 75\n",
        "hidden_size = 64\n",
        "\n",
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\n",
        "decoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\n",
        "\n",
        "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6T77re4_e-C"
      },
      "source": [
        "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
        "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
        "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
        "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
        "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
        "\n",
        "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
        "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :',len(encoder_input_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENvyNqYMrHpA"
      },
      "source": [
        "Model & Training - Bidirectional LSTM, Attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EaBSCKdrDpX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YwqCtTTQXi0"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/dataset/생성요약/training_3/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HWO85kirNrT"
      },
      "source": [
        "# 인코더\n",
        "encoder_inputs = Input(shape=(text_max_len,))\n",
        "\n",
        "# 인코더의 임베딩 층\n",
        "enc_emb = Embedding(src_vocab, 256)(encoder_inputs)\n",
        "\n",
        "# 인코더의 LSTM 1\n",
        "encoder_lstm1 = LSTM(64, return_sequences=True, return_state=True ,dropout = 0.2, recurrent_dropout = 0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# 인코더의 LSTM 2\n",
        "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# 인코더의 LSTM 3\n",
        "encoder_lstm3 = LSTM(64, return_state=True, return_sequences=True, dropout=0.2, recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# 디코더\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# 디코더의 임베딩 층\n",
        "dec_emb_layer = Embedding(tar_vocab, 200)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 디코더의 LSTM\n",
        "decoder_lstm = LSTM(64, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuFW3Ii1z2SK"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
        "from attention import AttentionLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2iYv3pNz6U5"
      },
      "source": [
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
        "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0tOF67A0Cmd"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
        "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train,\n",
        "          batch_size = 256, callbacks=[es, cp_callback], epochs = 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUyyJKDETZs8"
      },
      "source": [
        "Re-train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0gaURGybiS1"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/dataset/생성요약/training_6/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "model.load_weights(checkpoint_path) # 가중치 불러오기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ee_uAhQTYe6"
      },
      "source": [
        "checkpoint_path2 = \"/content/drive/MyDrive/dataset/생성요약/training_6/cp.ckpt\"\n",
        "checkpoint_dir2 = os.path.dirname(checkpoint_path2)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
        "cp_callback2 = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path2,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train,\n",
        "          batch_size = 256, callbacks=[es, cp_callback2], epochs = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYDcBD_WBNhj"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPGi8ByIBMS5"
      },
      "source": [
        "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
        "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
        "tar_index_to_word = tar_tokenizer.index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpfBTsRxBMay"
      },
      "source": [
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IupdvUGBMdF"
      },
      "source": [
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(64,))\n",
        "decoder_state_input_c = Input(shape=(64,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
        "# 훈련 과정에서와 달리 LSTM이 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j00Ol0gjBMft"
      },
      "source": [
        "# 어텐션 함수\n",
        "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
        "\n",
        "# 최종 디코더 모델\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUZ_KustBMke"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "     # <SOS>에 해당하는 토큰 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = tar_index_to_word[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='eostoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 상태를 업데이트 합니다.\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__kUzP-xBMnM"
      },
      "source": [
        "from tqdm import tqdm\n",
        "sub_df = []\n",
        "for i in tqdm(range(len(encoder_input_test))):\n",
        "    sub_df.append(decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGWVAZrCIZPz"
      },
      "source": [
        "sub_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na5wQZ1lBMsA"
      },
      "source": [
        "sub = pd.read_csv('/content/drive/MyDrive/dataset/생성요약/abstractive_sample_submission_v2.csv', index_col=0)\n",
        "sub['summary'] = sub_df\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWtHheSCCTg2"
      },
      "source": [
        "sub.to_csv('/content/drive/MyDrive/dataset/생성요약/abstractive_sample_submission_v2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}